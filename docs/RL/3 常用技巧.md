# 3 常用技巧

## 3.1 广义优势估计 GAE

**作用**：平衡TD与MC，即平衡方差与偏差

**形式**：$A_t^{GAE(\gamma, \lambda)}=\sum_{l=0}^{\infty}(\gamma\lambda)^{l}\delta_{t+l}$

**特例**：

- $\lambda=0$：$A_t^{GAE(\gamma, 0)}=\delta_{t+l}$，等价于TD Error
- $\lambda=1$：$A_t^{GAE(\gamma, 0)}=\sum_{l=0}^\infty \gamma^l \delta_{t+l}=\sum_{l=0}^\infty \gamma^l r_{t+l}-V(s_t)$，等价于MC

## 3.2 梯度裁剪

虽然Actor网络对新旧策略的差异进行了clip，但是在实际训练过程中仍然希望更新的步长不要太大，并且Critic网络没有任何保证，因此对Actor和Critic网络进行梯度裁剪是必要的。

## 3.3 熵正则

对于离散动作任务，熵正则能够使动作的不确定性增大，有利于Agent的探索。

## 3.4 Mini-Batch更新

## 3.5 重要性采样

**作用**：在求解$\mathbb{E}_{x\sim p}[f(x)]$时，如果$p(x)$较为复杂难以进行积分计算，可以尝试使用蒙特卡洛法进行近似积分。然而如果$X$无法在$p(x)$采样时，可以考虑使用重要性采样对偏差进行纠正。

**形式**：

$$
\mathbb{E}_{x\sim \color{red}{p}}[f(x)]=\mathbb{E}_{x\sim \color{green}{q}}\left[ \frac{p(x)}{q(x)}\cdot f(x) \right]
$$

**推导**：

$$
\begin{aligned}
\mathbb{E}_{x\sim \color{red}{p}}[f(x)]
&= \int p(z) f(z) \mathrm{d}z\\
&= \int q(z)\frac{p(z)}{q(z)}f(z)\mathrm{d}z \\
&=\mathbb{E}_{x\sim \color{green}{q}}\left[ \frac{p(x)}{q(x)}\cdot f(x) \right]\\
& \approx \frac{1}{N}\sum_{x_i\sim q, \ i=1}^N\frac{p(x)}{q(x)}f(x)
\end{aligned}
$$

**性质**：

$$
\begin{aligned}
\text{Var}[X]&=\mathbb{E}[X^2]-(\mathbb{E}[X])^2\\
\text{Var}_{x\sim q}\left[ \frac{p(x)}{q(x)}f(x) \right]
&=\mathbb{E}_{x\sim q}\left[ \left(\frac{p(x)}{q(x)}f(x)\right)^2 \right]-\left(\mathbb{E}_{x\sim q}\left[ \frac{p(x)}{q(x)}f(x) \right]\right)^2\\
&=\mathbb{E}_{x\sim \color{red}{p}}\left[ \frac{p(x)}{q(x)}f(x)^2 \right]-\left(\mathbb{E}_{x\sim \color{red}{p}}\left[ f(x) \right]\right)^2\\
\end{aligned}
$$

## 3.6 重要性采样 & Off-Policy

## 3.7 Gumbel Softmax
